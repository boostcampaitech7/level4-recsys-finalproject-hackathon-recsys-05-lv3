# config.yaml
memo: 
    cold start item baseline
# 아래의 일곱 항목들은 argparser로 받은 인자를 우선적으로 사용합니다.
#   $ python main.py --config config.yaml --seed 2024
#   과 같이 실행할 경우 seed는 0이 아닌 2024로 설정됩니다.


seed: 0         # 시드 고정
device: cuda    # 가능한 값 : cpu, cuda, mps
model: LightGCN       # 모델 선택
model_experiment_name : 'cold_item_baseline'  # 로컬에 저장될 이름

wandb: True                            # wandb 사용 여부
wandb_project: 'None'               # wandb 프로젝트 이름
wandb_experiment_name: 'None'            # wandb 실행 이름. 빈 문자열일 경우 자동 생성

tensorboard: True

model_args:
    LightGCN:
        latent_dim_rec: 64              # the embedding size of lightGCN
        n_layers: 3                 # the layer num of lightGCN
        dropout: 0               # using the dropout or not (0 or 1)
        keep_prob : 0.6
        multicore: 0             # whether to use multiprocessing in testing (0 or 1)
        pretrain: 0              # whether to use pretrained weights (0 or 1)
        A_split : True
        bigdata : True
    CLCRec:
        latent_dim_rec: 64               # the embedding size of lightGCN
        n_layers: 3                 # the layer num of lightGCN
        dropout: 0               # using the dropout or not (0 or 1)
        keep_prob : 0.6
        multicore: 0             # whether to use multiprocessing in testing (0 or 1)
        pretrain: 0              # whether to use pretrained weights (0 or 1)
        A_split : True
        bigdata : True
        tau: 0.5
        contrastive_weight: 0.1
        
dataset :
  data_dir : data/
  data : MovieLens32M
  preprocess_dir : /preprocessed

dataloader:
    n_fold : 100
    split : True
    bpr_batch_size: 4096    # 배치 사이즈
    test_batch_size : 128
    test_cold_batch_size : 32
    neg_ratio : 1
    threshold: 4
    timestamp: "2017-01-01"


loss: BPRLoss          # BPRLoss, BPRLossWithReg

optimizer:
    type: Adam      # 사용가능한 optimizer: torch.optim.Optimizer 클래스 (https://pytorch.org/docs/stable/optim.html#algorithms)
    args:           # 사용하고자 하는 클래스의 파라미터를 참고하여 추가해주시면 되며, 관계가 없는 파라미터는 무시됩니다.
        lr: 0.001             # 예) 모든 옵티마이저에서 사용되는 학습률
        weight_decay: 0.0001  # 예) Adam 등 / L2 정규화 가중치
        amsgrad: False      # 예) Adam 등 / amsgrad 사용 여부

metrics: ["recall", "ndcg", "mrr", "hr"]  # 원하는 지표 리스트

topks : [20]                         # k 에 대한 value  20이면 k = 20, 지표가  여러개 나옴.

train:
    epochs: 30                          # 학습 에폭 수
    resume: False                       # 이어서 학습할 경우 True
    save_interval: 10                   # 몇 에폭마다 모델 저장할지
    show_interval: 50                    # 몇 에폭마다 보일지